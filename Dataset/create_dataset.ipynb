{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter document with Grobid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All keys in the data:\n",
      "TEI\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Program to convert an xml\n",
    "# file to json file\n",
    " \n",
    "# import json module and xmltodict\n",
    "# module provided by python\n",
    "import json\n",
    "import xmltodict\n",
    " \n",
    " \n",
    "# open the input xml file and read\n",
    "# data in form of python dictionary \n",
    "# using xmltodict module\n",
    "with open(\"/home/grisel/Documents/internship/venv/test/s2.pdf.tei.xml\") as xml_file:\n",
    "     \n",
    "    data_dict = xmltodict.parse(xml_file.read())\n",
    "    # xml_file.close()\n",
    "     \n",
    "    # generate the object using json.dumps() \n",
    "    # corresponding to json data\n",
    "     \n",
    "    json_data = json.dumps(data_dict)\n",
    "     \n",
    "    # Write the json data to output \n",
    "    # json file\n",
    "    with open(\"data.json\", \"w\") as json_file:\n",
    "        json_file.write(json_data)\n",
    "        json_file.close()\n",
    "\n",
    "# Parse the JSON string\n",
    "data = json.loads(json_data)\n",
    "\n",
    "# Get all the keys in the data\n",
    "all_keys = data.keys()\n",
    "\n",
    "# Print the keys\n",
    "print(\"All keys in the data:\")\n",
    "for key in all_keys:\n",
    "    print(key)\n",
    "\n",
    "\n",
    "\n",
    "# Function to extract and write specified sections to a text file\n",
    "def write_sections_content_to_file(diccionario, sections_to_extract, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        # Recursive function to search for the specified sections\n",
    "        def extract_section_content(dic, depth=0):\n",
    "            for section in sections_to_extract:\n",
    "                if section in dic:\n",
    "                    file.write(f\"{'  ' * depth}{section.capitalize()} Content:\\n\")\n",
    "                    if isinstance(dic[section], dict):\n",
    "                        for key, value in dic[section].items():\n",
    "                            file.write(f\"{'  ' * (depth + 1)}{key}: {value}\\n\")\n",
    "                    else:\n",
    "                        file.write(f\"{'  ' * (depth + 1)}{dic[section]}\\n\")\n",
    "            for value in dic.values():\n",
    "                if isinstance(value, dict):\n",
    "                    extract_section_content(value, depth + 1)\n",
    "        # Call the recursive function to extract and write section content\n",
    "        extract_section_content(diccionario)\n",
    "\n",
    "\n",
    "# Define the sections to extract\n",
    "sections_to_extract = ['title', 'abstract', 'body']\n",
    "\n",
    "# Call the function to extract specified sections and write them to a file\n",
    "write_sections_content_to_file(data, sections_to_extract, '/home/grisel/Downloads/content_2/1-s2.0-S000632071100245X-main.pdf.tei.xml.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All keys in the data:\n",
      "TEI\n"
     ]
    }
   ],
   "source": [
    "# Parse the JSON string\n",
    "data = json.loads(json_data)\n",
    "\n",
    "# Get all the keys in the data\n",
    "all_keys = data.keys()\n",
    "\n",
    "# Print the keys\n",
    "print(\"All keys in the data:\")\n",
    "for key in all_keys:\n",
    "    print(key)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to try just in one tocument\n",
    "# Function to extract and write specified sections to a text file\n",
    "def write_sections_content_to_file(diccionario, sections_to_extract, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        # Recursive function to search for the specified sections\n",
    "        def extract_section_content(dic, depth=0):\n",
    "            for section in sections_to_extract:\n",
    "                if section in dic:\n",
    "                    file.write(f\"{'  ' * depth}{section.capitalize()} Content:\\n\")\n",
    "                    if isinstance(dic[section], dict):\n",
    "                        for key, value in dic[section].items():\n",
    "                            file.write(f\"{'  ' * (depth + 1)}{key}: {value}\\n\")\n",
    "                    else:\n",
    "                        file.write(f\"{'  ' * (depth + 1)}{dic[section]}\\n\")\n",
    "            for value in dic.values():\n",
    "                if isinstance(value, dict):\n",
    "                    extract_section_content(value, depth + 1)\n",
    "        # Call the recursive function to extract and write section content\n",
    "        extract_section_content(diccionario)\n",
    "\n",
    "\n",
    "# Define the sections to extract\n",
    "sections_to_extract = ['title', 'abstract', 'body']\n",
    "\n",
    "# Call the function to extract specified sections and write them to a file\n",
    "write_sections_content_to_file(data, sections_to_extract, '/home/grisel/Documents/internship/venv/test/s2.pdf.tei.xml.txt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to try all documents from one folder in format .tei.xml and the output is .json\n",
    "import json\n",
    "import xmltodict\n",
    "import os\n",
    "\n",
    "def remove_xmlns_sections(data):\n",
    "    if isinstance(data, dict):\n",
    "        keys_to_delete = [key for key in data if 'xmlns' in key]\n",
    "        for key in keys_to_delete:\n",
    "            del data[key]\n",
    "        for key, value in data.items():\n",
    "            remove_xmlns_sections(value)\n",
    "    elif isinstance(data, list):\n",
    "        for item in data:\n",
    "            remove_xmlns_sections(item)\n",
    "\n",
    "def write_sections_content_to_file(diccionario, sections_to_extract, file_path):\n",
    "    with open(file_path, 'w') as file:\n",
    "        # Recursive function to search for the specified sections\n",
    "        def extract_section_content(dic, depth=0):\n",
    "            for section in sections_to_extract:\n",
    "                if section in dic:\n",
    "                    file.write(f\"{'  ' * depth}{section.capitalize()} Content:\\n\")\n",
    "                    if isinstance(dic[section], dict):\n",
    "                        for key, value in dic[section].items():\n",
    "                            file.write(f\"{'  ' * (depth + 1)}{key}: {value}\\n\")\n",
    "                    else:\n",
    "                        file.write(f\"{'  ' * (depth + 1)}{dic[section]}\\n\")\n",
    "            for value in dic.values():\n",
    "                if isinstance(value, dict):\n",
    "                    extract_section_content(value, depth + 1)\n",
    "        # Call the recursive function to extract and write section content\n",
    "        extract_section_content(diccionario)\n",
    "\n",
    "def convert_xml_to_json_and_extract_sections(folder_path, sections_to_extract, output_folder):\n",
    "    # Ensure the output folder exists\n",
    "    os.makedirs(output_folder, exist_ok=True)\n",
    "\n",
    "    # List all XML files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.xml'):\n",
    "            xml_file_path = os.path.join(folder_path, filename)\n",
    "\n",
    "            # Read and convert XML to JSON\n",
    "            with open(xml_file_path) as xml_file:\n",
    "                data_dict = xmltodict.parse(xml_file.read())\n",
    "                json_data = json.dumps(data_dict)\n",
    "\n",
    "            # Parse the JSON string\n",
    "            data = json.loads(json_data)\n",
    "\n",
    "            # Remove unwanted sections\n",
    "            remove_xmlns_sections(data)\n",
    "\n",
    "            # Write the JSON data to an output JSON file (optional)\n",
    "            json_output_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.json\")\n",
    "            with open(json_output_path, 'w') as json_file:\n",
    "                json.dump(data, json_file, indent=4)\n",
    "\n",
    "            # Extract specified sections and write to a text file\n",
    "            text_output_path = os.path.join(output_folder, f\"{os.path.splitext(filename)[0]}.txt\")\n",
    "            write_sections_content_to_file(data, sections_to_extract, text_output_path)\n",
    "\n",
    "# Define the folder containing the XML files and the output folder for the results\n",
    "input_folder = '/home/grisel/Downloads/tei corpus'\n",
    "output_folder = '/home/grisel/Downloads/content_4'\n",
    "sections_to_extract = ['title', 'abstract', 'body']\n",
    "\n",
    "# Process all XML files in the input folder\n",
    "convert_xml_to_json_and_extract_sections(input_folder, sections_to_extract, output_folder)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import PyPDF2\n",
    "import os\n",
    "import spacy\n",
    "import re\n",
    "import csv\n",
    "\n",
    "def extract_text(file_path):\n",
    "    text = ''\n",
    "    with open(file_path, 'r') as file:\n",
    "        text = file.read()\n",
    "    \n",
    "    return(text)\n",
    "\n",
    "\n",
    "def text_preparation(text):\n",
    "    # Load the English language model\n",
    "    nlp = spacy.load('en_core_web_sm')\n",
    "    nlp.max_length = 2000000  # Set max_length limit to 2,000,000 characters\n",
    "        \n",
    "        # Tokenization, stop words removal, and lemmatization\n",
    "    doc = nlp(text)\n",
    "        \n",
    "        # Process the text and reconstruct sentences\n",
    "    sentences = []\n",
    "    for sent in doc.sents:\n",
    "        tokens = [token.lemma_ for token in sent if not token.is_stop and not token.is_punct]\n",
    "        if tokens:\n",
    "            sentence = ' '.join(tokens)\n",
    "            sentences.append(sentence)\n",
    "        \n",
    "        # Join the processed sentences with a dot and space\n",
    "    processed_text = '. '.join(sentences) + '.'\n",
    "        \n",
    "    return processed_text\n",
    "\n",
    "def split_into_segments(text):\n",
    "    # Split text into segments based on sentence boundaries (periods followed by whitespace)\n",
    "    segments = re.split(r'\\.+\\s', text)\n",
    "    return segments\n",
    "\n",
    "def read_csv(file_name):\n",
    "    words = []\n",
    "    with open(file_name, 'r') as file:\n",
    "        csv_reader = csv.reader(file)\n",
    "        for row in csv_reader:\n",
    "            for word in row:\n",
    "                if word:  # Check if word is not empty\n",
    "                    words.append(word)\n",
    "    return words\n",
    "\n",
    "\n",
    "\n",
    "def save_segments_with_word(segments, word_list, output_file, highlight_char='[', end_highlight_char=']'):\n",
    "    highlighted_segments = []\n",
    "    \n",
    "    for segment in segments:\n",
    "        contains_word = False\n",
    "        highlighted_segment = segment\n",
    "        for word in word_list:\n",
    "            if re.search(fr'(?i)\\b{re.escape(word)}\\b', segment):\n",
    "                contains_word = True\n",
    "                # Use a case-insensitive regex to find and replace all occurrences of the word\n",
    "                highlighted_segment = re.sub(\n",
    "                    fr'(?i)\\b{re.escape(word)}\\b', \n",
    "                    f\"{highlight_char}\\\\g<0>{end_highlight_char}\", \n",
    "                    highlighted_segment\n",
    "                )\n",
    "        if contains_word:\n",
    "            highlighted_segments.append([highlighted_segment])  # Append as a list to make it a single column CSV row\n",
    "    \n",
    "    # Write to CSV file\n",
    "    with open(output_file, 'w', newline='') as file:\n",
    "        writer = csv.writer(file)\n",
    "        writer.writerow(['Segment'])  # Write header\n",
    "        writer.writerows(highlighted_segments)\n",
    "\n",
    "\n",
    "\n",
    "text = extract_text('/home/grisel/Documents/internship/venv/test/s2.pdf.tei.xml.txt')  \n",
    "text_preprocess = text\n",
    "word_list = read_csv('/home/grisel/Documents/internship/venv/NLPinternship-TETIS-HSM/test/nomenclature_v3.csv')\n",
    "segments = split_into_segments(text_preprocess) \n",
    "save_segments_with_word(segments, word_list, '/home/grisel/Documents/internship/venv/test/s2.pdf.tei.xml.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Define the directory containing the files\n",
    "directory = '/home/grisel/Downloads/content_4'\n",
    "\n",
    "# Loop over each file in the directory\n",
    "for filename in os.listdir(directory):\n",
    "    if filename.endswith('.txt'):  # Consider only text files\n",
    "        file_path = os.path.join(directory, filename)\n",
    "        \n",
    "        # Perform text processing for each file\n",
    "        text = extract_text(file_path)\n",
    "        text_preprocess = text\n",
    "        word_list = read_csv('/home/grisel/Documents/internship/venv/NLPinternship-TETIS-HSM/test/nomenclature_v3.csv')\n",
    "        segments = split_into_segments(text_preprocess)\n",
    "        \n",
    "        # Save processed segments to a CSV file\n",
    "        output_file = os.path.join('/home/grisel/Downloads/csv_text', f\"{os.path.splitext(filename)[0]}.csv\")\n",
    "        save_segments_with_word(segments, word_list, output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "def merge_and_shuffle_csv(folder_path, output_file):\n",
    "    all_segments = []\n",
    "\n",
    "    # List all CSV files in the folder\n",
    "    for filename in os.listdir(folder_path):\n",
    "        if filename.endswith('.csv'):\n",
    "            file_path = os.path.join(folder_path, filename)\n",
    "            df = pd.read_csv(file_path)\n",
    "            \n",
    "            # Ensure there is a 'Segment' column in the CSV file\n",
    "            if 'Segment' in df.columns:\n",
    "                # Extract document name from filename\n",
    "                document_name = os.path.splitext(filename)[0]\n",
    "                # Add document name and segment number to each segment\n",
    "                df['Document'] = document_name\n",
    "                df['Segment_Num'] = df.groupby('Document').cumcount() + 1\n",
    "                df['Document_Segment'] = df['Document'] + '_' + df['Segment_Num'].astype(str)\n",
    "                all_segments.extend(df[['Document_Segment', 'Segment']].values.tolist())\n",
    "\n",
    "    # Shuffle the collected segments\n",
    "    random.shuffle(all_segments)\n",
    "\n",
    "    # Save the shuffled segments to a new CSV file\n",
    "    output_df = pd.DataFrame(all_segments, columns=['Document_Segment', 'Segment'])\n",
    "    output_df.to_csv(output_file, index=False)\n",
    "\n",
    "# Example usage:\n",
    "folder_path = '/home/grisel/Downloads/csv_text'\n",
    "output_file = '/home/grisel/Downloads/output/merged_shuffled_output_3.csv'\n",
    "\n",
    "merge_and_shuffle_csv(folder_path, output_file)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
